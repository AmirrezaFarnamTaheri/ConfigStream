name: ConfigStream Pipeline

on:
  push:
    branches: [main]
  pull_request:
  schedule:
    - cron: "0 */4 * * *"
  workflow_dispatch:

permissions:
  contents: write

jobs:
  lint:
    name: Lint and Static Analysis
    runs-on: ubuntu-latest
    continue-on-error: false
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip
          cache-dependency-path: pyproject.toml

      - name: Install dependencies
        run: pip install -e .[dev]

      - name: Run Linters
        run: |
          black --check .
          flake8 .
          mypy .

  test:
    name: Run Test Suite
    runs-on: ubuntu-latest
    continue-on-error: false
    env:
      PYTHONPATH: src
      MAXMIND_LICENSE_KEY: ${{ secrets.MAXMIND_LICENSE_KEY }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip
          cache-dependency-path: pyproject.toml

      - name: Install dependencies
        run: pip install -e .[dev]

      - name: Cache sing-box binary
        id: cache-sing-box
        uses: actions/cache@v3
        with:
          path: /usr/local/bin/sing-box
          key: sing-box-linux-amd64-${{ github.sha }}
          restore-keys: |
            sing-box-linux-amd64-

      - name: Install sing-box
        if: steps.cache-sing-box.outputs.cache-hit != 'true'
        run: |
          LATEST_URL=$(curl -s "https://api.github.com/repos/SagerNet/sing-box/releases/latest" | grep "browser_download_url.*linux-amd64" | cut -d '"' -f 4)
          curl -L -o sing-box.tar.gz $LATEST_URL
          tar -xzf sing-box.tar.gz
          # The extracted directory name can vary, so we find it dynamically
          EXTRACTED_DIR=$(tar -tzf sing-box.tar.gz | head -1 | cut -f1 -d"/")
          mv $EXTRACTED_DIR/sing-box /usr/local/bin/
          rm -rf sing-box.tar.gz $EXTRACTED_DIR

      - name: Run tests
        run: python -m pytest --cov=src/configstream --cov-report=term-missing

  pipeline:
    name: Run Rotational Batch Pipeline
    runs-on: ubuntu-latest
    needs: [lint, test]
    env:
      PYTHONPATH: src
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          # Fetch all history so we can diff against the previous commit
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip
          cache-dependency-path: pyproject.toml

      - name: Restore test cache and GeoIP database
        uses: actions/cache@v3
        with:
          path: |
            data/test_cache.db
            data/GeoLite2-City.mmdb
            output/.etag-cache.json
          key: configstream-cache-${{ github.run_number }}
          restore-keys: |
            configstream-cache-

      - name: Install dependencies
        run: pip install -e .[dev]

      - name: Cache sing-box binary
        id: cache-sing-box
        uses: actions/cache@v3
        with:
          path: /usr/local/bin/sing-box
          key: sing-box-linux-amd64-${{ github.sha }}
          restore-keys: |
            sing-box-linux-amd64-

      - name: Install sing-box
        if: steps.cache-sing-box.outputs.cache-hit != 'true'
        run: |
          LATEST_URL=$(curl -s "https://api.github.com/repos/SagerNet/sing-box/releases/latest" | grep "browser_download_url.*linux-amd64" | cut -d '"' -f 4)
          curl -L -o sing-box.tar.gz $LATEST_URL
          tar -xzf sing-box.tar.gz
          # The extracted directory name can vary, so we find it dynamically
          EXTRACTED_DIR=$(tar -tzf sing-box.tar.gz | head -1 | cut -f1 -d"/")
          mv $EXTRACTED_DIR/sing-box /usr/local/bin/
          rm -rf sing-box.tar.gz $EXTRACTED_DIR

      - name: Download GeoIP databases
        env:
          MAXMIND_LICENSE_KEY: ${{ secrets.MAXMIND_LICENSE_KEY }}
        run: python -m configstream.cli update-databases

      - name: Determine which batch to run
        id: batch_selector
        run: |
          set -euxo pipefail
          # This will result in a number from 1 to 6, rotating every 4 hours.
          # (0, 4, 8, 12, 16, 20) -> (1, 2, 3, 4, 5, 6)
          # By using 10# prefix, we force bash to interpret the hour as a base-10 number,
          # avoiding the "value too great for base" error for hours like 08 and 09.
          batch_number=$(( ( 10#$(date -u +%H) / 4 ) + 1 ))
          echo "batch_number=${batch_number}" >> $GITHUB_OUTPUT

      - name: Run pipeline for Batch ${{ steps.batch_selector.outputs.batch_number }}
        env:
          BATCH_NUMBER: ${{ steps.batch_selector.outputs.batch_number }}
        run: |
          set -euxo pipefail
          echo "Running pipeline for batch $BATCH_NUMBER"
          python -m configstream.cli merge \
            --sources "sources/batch_${BATCH_NUMBER}.txt" \
            --output "output_batch_${BATCH_NUMBER}" \
            --max-workers 32 \
            --timeout 24 \
            --show-metrics \
            --max-latency 6000 | tee "pipeline_batch_${BATCH_NUMBER}.log"

      - name: Run merge script
        run: |
          echo "Merging results from all batches..."
          python -m scripts.merge_batches

      - name: Show diff
        run: |
          git status --short
          git diff --stat || true

      - name: Commit updated output
        if: github.event_name != 'pull_request' && github.ref == 'refs/heads/main'
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore(data): update batch ${{ steps.batch_selector.outputs.batch_number }} and merge outputs"
          # We need to add all batch outputs to preserve their state, plus the final merged output.
          file_pattern: |
            output/
            output_batch_*/
          push_options: '--force'
          add_options: '-f'

      - name: Upload pipeline artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-run-${{ github.sha }}
          path: |
            output/
            output_batch_*/
            *.log
